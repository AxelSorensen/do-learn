{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DoLearn: Causal Discovery through Interventions Welcome to this project, to get started quickly go to Quickstart","title":"DoLearn: Causal Discovery through Interventions"},{"location":"#dolearn-causal-discovery-through-interventions","text":"Welcome to this project, to get started quickly go to Quickstart","title":"DoLearn: Causal Discovery through Interventions"},{"location":"quickstart/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Quickstart Defining causal model Let's start by instantiating a causal model and providing nodes and edges as arguments. import os os.chdir(os.path.abspath(os.path.join('..'))) from do_learn import CausalGraph from do_learn import generate_dag, generate_all_dags, diff_to_edge_list from do_learn import EquivalenceClass import random nodes = ['A', 'B', 'C'] edges = [('A', 'B'), ('B', 'C')] graph = CausalGraph(nodes,edges) graph.display() We can also generate a random causal model by calling generate_dag() and providing the number of nodes and edge probability. from do_learn.utils import generate_dag random_graph = generate_dag(num_nodes=5, edge_prob=.8) random_graph.display() Now let us sample from our first graph using the sample() function graph.sample() {'A': 0, 'B': 0, 'C': 0} Since no initial values were given each variable deafults to 0. We can carry out an intervention on a variable by calling the do() function. It returns a dictionary with 3 keys (pre, post, diff), represeting the values of each variable before and after the intervention and finally the difference between these values. graph.do('A', 1) {'pre': {'A': 0, 'B': 0, 'C': 0}, 'post': {'A': 1, 'B': 1.0, 'C': 1.0}, 'diff': {'A': {'B': 1.0, 'C': 1.0}}} In our simple example A causes B which causes C, so when A is set to 1 the other two variables change as well. Visualizing possible graphs The previous example illustrated sampling from a known causal graph and inferring the results of interventions, however in many cases we are interested in learning an unknown causal graph and using interventions to probe for causal relations. Let's start with the simple case of two variables X and Y. Given two variables, there exist three possible causal structures. Either X causes Y, Y causes X or X and Y are independent. DoLearn allows us to represent these possible graphs using the EquivalenceClass class. We can plot each one of these graphs using the display_all() function or simply call display_essential_graph() to represent all graphs in one true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() A dashed undirected edge represent that the edge either goes from X0 to X1, from X1 to X0 or doesn't exist (making X0 and X1 independent) and thereby visualizes all three possible graphs at once. If we slice the possible graphs to only include 2 of them we get a different essential graph. Now we see a directed dashed edge from X0 to X1, representing that the edge either X0 to X1 or doesn't exist. true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs[:2]) equivalence_class.display_essential_graph() Learning When learning a causal graph we want to reduce the equivalence class of possible graphs by carrying out interventions and ruling out models that aren't consistent with our data. Hopefully with enough interventions we will end up with the true underlying structure. Let's try learning the true graph in 3-variable setting. First we will define the true causal graph as follows: true_graph = CausalGraph(['A','B','C'], [('A','B'),('A','C'),('B','C')]) possible_graphs = generate_all_dags(['A','B','C']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() With three variables there are 25 possible graphs. In order to learn the causal structure we will carry out an intervention on one of the nodes and see what changes. Based on the difference between the pre intervention distribution and the post intervention distribution we decide which graphs to keep and remove from the equivalence class. Let's start by intervening on A intervention = true_graph.do('A',1) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs(keep, remove,True) equivalence_class.display_essential_graph() Since both B and C change as a result of A, the true graph must have a directed edge from A to C and from A to B (EXCEPTION: can't tell diff between ancestor and parent). We rule out all graphs were this is not the case and reduce our equivalence class from 25 to only 3 graphs. Next let us carry out an intervention on C intervention = true_graph.do('C',random.random()) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs(keep, remove,True) equivalence_class.display_essential_graph() Since B does not change as a result of an intervention on C, we can rule out all graphs were there is a directed edge from C to B. This leaves us with only 2 possible graphs. Finally we need to carry out an intervention on B to reduce the equivalence class even further intervention = true_graph.do('B',random.random()) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs(keep, remove,True) equivalence_class.display_essential_graph() We end up with the true causal graph. We have succesfully learned it through our interventions","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#defining-causal-model","text":"Let's start by instantiating a causal model and providing nodes and edges as arguments. import os os.chdir(os.path.abspath(os.path.join('..'))) from do_learn import CausalGraph from do_learn import generate_dag, generate_all_dags, diff_to_edge_list from do_learn import EquivalenceClass import random nodes = ['A', 'B', 'C'] edges = [('A', 'B'), ('B', 'C')] graph = CausalGraph(nodes,edges) graph.display() We can also generate a random causal model by calling generate_dag() and providing the number of nodes and edge probability. from do_learn.utils import generate_dag random_graph = generate_dag(num_nodes=5, edge_prob=.8) random_graph.display() Now let us sample from our first graph using the sample() function graph.sample() {'A': 0, 'B': 0, 'C': 0} Since no initial values were given each variable deafults to 0. We can carry out an intervention on a variable by calling the do() function. It returns a dictionary with 3 keys (pre, post, diff), represeting the values of each variable before and after the intervention and finally the difference between these values. graph.do('A', 1) {'pre': {'A': 0, 'B': 0, 'C': 0}, 'post': {'A': 1, 'B': 1.0, 'C': 1.0}, 'diff': {'A': {'B': 1.0, 'C': 1.0}}} In our simple example A causes B which causes C, so when A is set to 1 the other two variables change as well.","title":"Defining causal model"},{"location":"quickstart/#visualizing-possible-graphs","text":"The previous example illustrated sampling from a known causal graph and inferring the results of interventions, however in many cases we are interested in learning an unknown causal graph and using interventions to probe for causal relations. Let's start with the simple case of two variables X and Y. Given two variables, there exist three possible causal structures. Either X causes Y, Y causes X or X and Y are independent. DoLearn allows us to represent these possible graphs using the EquivalenceClass class. We can plot each one of these graphs using the display_all() function or simply call display_essential_graph() to represent all graphs in one true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() A dashed undirected edge represent that the edge either goes from X0 to X1, from X1 to X0 or doesn't exist (making X0 and X1 independent) and thereby visualizes all three possible graphs at once. If we slice the possible graphs to only include 2 of them we get a different essential graph. Now we see a directed dashed edge from X0 to X1, representing that the edge either X0 to X1 or doesn't exist. true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs[:2]) equivalence_class.display_essential_graph()","title":"Visualizing possible graphs"},{"location":"quickstart/#learning","text":"When learning a causal graph we want to reduce the equivalence class of possible graphs by carrying out interventions and ruling out models that aren't consistent with our data. Hopefully with enough interventions we will end up with the true underlying structure. Let's try learning the true graph in 3-variable setting. First we will define the true causal graph as follows: true_graph = CausalGraph(['A','B','C'], [('A','B'),('A','C'),('B','C')]) possible_graphs = generate_all_dags(['A','B','C']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() With three variables there are 25 possible graphs. In order to learn the causal structure we will carry out an intervention on one of the nodes and see what changes. Based on the difference between the pre intervention distribution and the post intervention distribution we decide which graphs to keep and remove from the equivalence class. Let's start by intervening on A intervention = true_graph.do('A',1) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs(keep, remove,True) equivalence_class.display_essential_graph() Since both B and C change as a result of A, the true graph must have a directed edge from A to C and from A to B (EXCEPTION: can't tell diff between ancestor and parent). We rule out all graphs were this is not the case and reduce our equivalence class from 25 to only 3 graphs. Next let us carry out an intervention on C intervention = true_graph.do('C',random.random()) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs(keep, remove,True) equivalence_class.display_essential_graph() Since B does not change as a result of an intervention on C, we can rule out all graphs were there is a directed edge from C to B. This leaves us with only 2 possible graphs. Finally we need to carry out an intervention on B to reduce the equivalence class even further intervention = true_graph.do('B',random.random()) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs(keep, remove,True) equivalence_class.display_essential_graph() We end up with the true causal graph. We have succesfully learned it through our interventions","title":"Learning"}]}