{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DoLearn: Causal Discovery through Interventions Welcome to this project, to get started quickly go to Quickstart","title":"DoLearn: Causal Discovery through Interventions"},{"location":"#dolearn-causal-discovery-through-interventions","text":"Welcome to this project, to get started quickly go to Quickstart","title":"DoLearn: Causal Discovery through Interventions"},{"location":"quickstart/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Quickstart This quickstart guide should get you familiar with how to use DoLearn Defining causal model Let's start by instantiating a causal model and providing nodes and edges as arguments. import os os.chdir(os.path.abspath(os.path.join('..'))) from do_learn import CausalGraph from do_learn import generate_dag, generate_all_dags, diff_to_edge_list, minimal_certain_graph from do_learn import EquivalenceClass import random nodes = ['A', 'B', 'C'] edges = [('A', 'B'), ('B', 'C')] graph = CausalGraph(nodes,edges) graph.display() graph.do('A',1) {'pre': {'A': 0, 'B': 0, 'C': 0}, 'post': {'A': 1, 'B': 1.0, 'C': 1.0}, 'diff': {'A': {'B': 1.0, 'C': 1.0}}} We can also generate a random causal model by calling generate_dag() and providing the number of nodes and edge probability. from do_learn.utils import generate_dag random_graph = generate_dag(num_nodes=5, edge_prob=.8) random_graph.display() Now let us sample from our first graph using the sample() function graph.sample() {'A': 1, 'B': 1.0, 'C': 1.0} Since no initial values were given each variable deafults to 0. We can carry out an intervention on a variable by calling the do() function. It returns a dictionary with 3 keys (pre, post, diff), represeting the values of each variable before and after the intervention and finally the difference between these values. graph.do('A', 1) {'pre': {'A': 1, 'B': 1.0, 'C': 1.0}, 'post': {'A': 1, 'B': 1.0, 'C': 1.0}, 'diff': {'A': {'B': 0.0, 'C': 0.0}}} In our simple example A causes B which causes C, so when A is set to 1 the other two variables change as well. Visualizing possible graphs The previous example illustrated sampling from a known causal graph and inferring the results of interventions, however in many cases we are interested in learning an unknown causal graph and using interventions to probe for causal relations. Let's start with the simple case of two variables X and Y. Given two variables, there exist three possible causal structures. Either X causes Y, Y causes X or X and Y are independent. DoLearn allows us to represent these possible graphs using the EquivalenceClass class. We can plot each one of these graphs using the display_all() function or simply call display_essential_graph() to represent all graphs in one true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() A dashed undirected edge represent that the edge either goes from X0 to X1, from X1 to X0 or doesn't exist (making X0 and X1 independent) and thereby visualizes all three possible graphs at once. If we slice the possible graphs to only include 2 of them we get a different essential graph. Now we see a directed dashed edge from X0 to X1, representing that the edge either X0 to X1 or doesn't exist. true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs[:2]) equivalence_class.display_essential_graph() Learning When learning a causal graph we want to reduce the equivalence class of possible graphs by carrying out interventions and ruling out models that aren't consistent with our data. Hopefully with enough interventions we will end up with the true underlying structure. Let's try learning the true graph in 3-variable setting. First we will define the true causal graph as follows: true_graph = CausalGraph(['A','B','C'], [('A','B'),('A','C'),('B','C')]) possible_graphs = generate_all_dags(['A','B','C']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() With three variables there are 25 possible graphs. In order to learn the causal structure we will carry out an intervention on one of the nodes and see what changes. Based on the difference between the pre intervention distribution and the post intervention distribution we decide which graphs to keep and remove from the equivalence class. Let's start by intervening on A intervention = true_graph.do('A',1) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs('A',intervention['diff'],0,True) equivalence_class.display_essential_graph() Since both B and C change as a result of A, the true graph must have a directed edge from A to C and from A to B (EXCEPTION: can't tell diff between ancestor and parent). We rule out all graphs were this is not the case and reduce our equivalence class from 25 to only 3 graphs. Next let us carry out an intervention on C intervention = true_graph.do('C',random.random()) equivalence_class.filter_graphs('C',intervention['diff'],0,True) equivalence_class.display_essential_graph() Since B does not change as a result of an intervention on C, we can rule out all graphs were there is a directed edge from C to B. This leaves us with only 2 possible graphs. Finally we need to carry out an intervention on B to reduce the equivalence class even further intervention = true_graph.do('B',random.random()) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs('B',intervention['diff'],0,True) equivalence_class.display_essential_graph() We end up with the true causal graph. We have succesfully learned it through our interventions Minimal Certain Graph Due to how our causal learning algorithm works, we are not able to distinguish between direct parents and ancestors of a node. Take for example the following causal graph: A -> B -> C. true_graph = CausalGraph(['A','B','C'], [('A','B'),('B','C')]) possible_graphs = generate_all_dags(['A','B','C']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() An intervention on A causes a change in both B and C, leading us to draw a directed edge from A to B and from A to C. intervention = true_graph.do('A',random.random()) equivalence_class.filter_graphs('A',intervention['diff'],0,True) equivalence_class.display_essential_graph() If we then intervene on B we end up with only 2 graphs in our equivalence class. We are unsure about whether A causes C directly or through B. intervention = true_graph.do('B',random.random()) equivalence_class.filter_graphs('B',intervention['diff'],0,True) equivalence_class.display_essential_graph() In order to resolve this ambiguity we can make use of the minimal certain graph. A minimal certain graph is a subgraph that consists of the minimal number of edges that make this graph consistent with the observed effects of interventions. It works a bit like occam's razor, finding the graph in the equivalence class with least edges that still explains the data. We can use the minimal_certain_graph() function to find it. graph = minimal_certain_graph(equivalence_class) graph.display() The minimal certain graph can function as a heuristic to choose between graphs when in doubt. However, in this case the minimal certain graph does not match the true underlying graph. In order to infer the correct underlying graph with certainity we have to carry out interventions while holding the value of some variables fixed. Controlled Interventions Controlled interventions allow us to carry out do-interventions while keeping a set number of variables at a fixed value, in order to 'block' of these paths. To carry out a controlled intervention we need to provide a list of variable names to the fixed argument of the do() function. Let us once again display our equivalence class but instead of outputting the minimal certain graph we will fix the value of B and then intervene on A. This way we block possible influence of A on C through B. intervention = true_graph.do('A',random.random(),['B']) equivalence_class.filter_graphs('A',intervention['diff'],0,True,['B']) equivalence_class.display_essential_graph() Since no change in seen in C when holding B fixed and changing A we can rule out the graph with a direct edge from A to C.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This quickstart guide should get you familiar with how to use DoLearn","title":"Quickstart"},{"location":"quickstart/#defining-causal-model","text":"Let's start by instantiating a causal model and providing nodes and edges as arguments. import os os.chdir(os.path.abspath(os.path.join('..'))) from do_learn import CausalGraph from do_learn import generate_dag, generate_all_dags, diff_to_edge_list, minimal_certain_graph from do_learn import EquivalenceClass import random nodes = ['A', 'B', 'C'] edges = [('A', 'B'), ('B', 'C')] graph = CausalGraph(nodes,edges) graph.display() graph.do('A',1) {'pre': {'A': 0, 'B': 0, 'C': 0}, 'post': {'A': 1, 'B': 1.0, 'C': 1.0}, 'diff': {'A': {'B': 1.0, 'C': 1.0}}} We can also generate a random causal model by calling generate_dag() and providing the number of nodes and edge probability. from do_learn.utils import generate_dag random_graph = generate_dag(num_nodes=5, edge_prob=.8) random_graph.display() Now let us sample from our first graph using the sample() function graph.sample() {'A': 1, 'B': 1.0, 'C': 1.0} Since no initial values were given each variable deafults to 0. We can carry out an intervention on a variable by calling the do() function. It returns a dictionary with 3 keys (pre, post, diff), represeting the values of each variable before and after the intervention and finally the difference between these values. graph.do('A', 1) {'pre': {'A': 1, 'B': 1.0, 'C': 1.0}, 'post': {'A': 1, 'B': 1.0, 'C': 1.0}, 'diff': {'A': {'B': 0.0, 'C': 0.0}}} In our simple example A causes B which causes C, so when A is set to 1 the other two variables change as well.","title":"Defining causal model"},{"location":"quickstart/#visualizing-possible-graphs","text":"The previous example illustrated sampling from a known causal graph and inferring the results of interventions, however in many cases we are interested in learning an unknown causal graph and using interventions to probe for causal relations. Let's start with the simple case of two variables X and Y. Given two variables, there exist three possible causal structures. Either X causes Y, Y causes X or X and Y are independent. DoLearn allows us to represent these possible graphs using the EquivalenceClass class. We can plot each one of these graphs using the display_all() function or simply call display_essential_graph() to represent all graphs in one true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() A dashed undirected edge represent that the edge either goes from X0 to X1, from X1 to X0 or doesn't exist (making X0 and X1 independent) and thereby visualizes all three possible graphs at once. If we slice the possible graphs to only include 2 of them we get a different essential graph. Now we see a directed dashed edge from X0 to X1, representing that the edge either X0 to X1 or doesn't exist. true_graph = CausalGraph(['A','B'], [('A','B')]) possible_graphs = generate_all_dags(['A','B']) equivalence_class = EquivalenceClass(possible_graphs[:2]) equivalence_class.display_essential_graph()","title":"Visualizing possible graphs"},{"location":"quickstart/#learning","text":"When learning a causal graph we want to reduce the equivalence class of possible graphs by carrying out interventions and ruling out models that aren't consistent with our data. Hopefully with enough interventions we will end up with the true underlying structure. Let's try learning the true graph in 3-variable setting. First we will define the true causal graph as follows: true_graph = CausalGraph(['A','B','C'], [('A','B'),('A','C'),('B','C')]) possible_graphs = generate_all_dags(['A','B','C']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() With three variables there are 25 possible graphs. In order to learn the causal structure we will carry out an intervention on one of the nodes and see what changes. Based on the difference between the pre intervention distribution and the post intervention distribution we decide which graphs to keep and remove from the equivalence class. Let's start by intervening on A intervention = true_graph.do('A',1) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs('A',intervention['diff'],0,True) equivalence_class.display_essential_graph() Since both B and C change as a result of A, the true graph must have a directed edge from A to C and from A to B (EXCEPTION: can't tell diff between ancestor and parent). We rule out all graphs were this is not the case and reduce our equivalence class from 25 to only 3 graphs. Next let us carry out an intervention on C intervention = true_graph.do('C',random.random()) equivalence_class.filter_graphs('C',intervention['diff'],0,True) equivalence_class.display_essential_graph() Since B does not change as a result of an intervention on C, we can rule out all graphs were there is a directed edge from C to B. This leaves us with only 2 possible graphs. Finally we need to carry out an intervention on B to reduce the equivalence class even further intervention = true_graph.do('B',random.random()) keep, remove = diff_to_edge_list(intervention['diff']) equivalence_class.filter_graphs('B',intervention['diff'],0,True) equivalence_class.display_essential_graph() We end up with the true causal graph. We have succesfully learned it through our interventions","title":"Learning"},{"location":"quickstart/#minimal-certain-graph","text":"Due to how our causal learning algorithm works, we are not able to distinguish between direct parents and ancestors of a node. Take for example the following causal graph: A -> B -> C. true_graph = CausalGraph(['A','B','C'], [('A','B'),('B','C')]) possible_graphs = generate_all_dags(['A','B','C']) equivalence_class = EquivalenceClass(possible_graphs) equivalence_class.display_essential_graph() An intervention on A causes a change in both B and C, leading us to draw a directed edge from A to B and from A to C. intervention = true_graph.do('A',random.random()) equivalence_class.filter_graphs('A',intervention['diff'],0,True) equivalence_class.display_essential_graph() If we then intervene on B we end up with only 2 graphs in our equivalence class. We are unsure about whether A causes C directly or through B. intervention = true_graph.do('B',random.random()) equivalence_class.filter_graphs('B',intervention['diff'],0,True) equivalence_class.display_essential_graph() In order to resolve this ambiguity we can make use of the minimal certain graph. A minimal certain graph is a subgraph that consists of the minimal number of edges that make this graph consistent with the observed effects of interventions. It works a bit like occam's razor, finding the graph in the equivalence class with least edges that still explains the data. We can use the minimal_certain_graph() function to find it. graph = minimal_certain_graph(equivalence_class) graph.display() The minimal certain graph can function as a heuristic to choose between graphs when in doubt. However, in this case the minimal certain graph does not match the true underlying graph. In order to infer the correct underlying graph with certainity we have to carry out interventions while holding the value of some variables fixed.","title":"Minimal Certain Graph"},{"location":"quickstart/#controlled-interventions","text":"Controlled interventions allow us to carry out do-interventions while keeping a set number of variables at a fixed value, in order to 'block' of these paths. To carry out a controlled intervention we need to provide a list of variable names to the fixed argument of the do() function. Let us once again display our equivalence class but instead of outputting the minimal certain graph we will fix the value of B and then intervene on A. This way we block possible influence of A on C through B. intervention = true_graph.do('A',random.random(),['B']) equivalence_class.filter_graphs('A',intervention['diff'],0,True,['B']) equivalence_class.display_essential_graph() Since no change in seen in C when holding B fixed and changing A we can rule out the graph with a direct edge from A to C.","title":"Controlled Interventions"}]}